---
layout: post
title:  "Normalized Estimation Error Squared (NEES)"
permalink: /normalized-estimation-error-squared/
date:   2022-08-14 00:00:00 +0000
categories: kalman-filter
---

A desired property of a state estimator is consistency. 
A state estimator is consistent if it is able to correctly indicate the quality of the estimate. 
It should be able to do this because an increase in sample size leads to a growth in information content and the state estimate \\( \hat{\mathbf{x}}(k) \\) is as close as possible to the true state \\( \mathbf{x}(k)  \\).
This results from the requirement that a state estimator shall be unbiased.
Mathematically speaking, this is expressed by the expected value of the estimation error \\( \tilde{\mathbf{x}}(k)  \\) being zero

\\[ E [ \mathbf{x}(k) - \hat{\mathbf{x}}(k)  ] = E [ \tilde{\mathbf{x}}(k)  ] = 0 \\]

and that the covariance matrix of the estimation error is equal to the covariance matrix of the state estimator

\\[ E [ [\mathbf{x}(k) - \hat{\mathbf{x}}(k)] [\mathbf{x}(k) - \hat{\mathbf{x}}(k)]^T ] = E [ \tilde{\mathbf{x}}(k\|k) \tilde{\mathbf{x}}(k\|k)^T  ] = \mathbf{P}(k\|k) \. \\]

Checking the consistency of the state estimator can be done with a hypothesis test using the Normalized Estimation Error Squared (NEES).
For this purpose, a normalization of the estimation error \\( \tilde{\mathbf{x}} \\) and the error covariance matrix \\( \mathbf{P} \\) is performed

\\[ \epsilon (k) = \tilde{\mathbf{x}}(k\|k)^T \mathbf{P}(k\|k)^{-1} \tilde{\mathbf{x}}(k\|k) \ .\\]

The quantity \\( \epsilon (k) \\)  is chi-squared distributed with \\( \text{dim}( \tilde{\mathbf{x}}(k) )) = n_x \\) degrees of freedom.

*Brief explanation why chi-squared distributed: The sum of squared random variables which are standard normally distributed are chi-squared distributed. 
Under the assumption that \\( \mathbf{x}(k) \\) and \\( \hat{\mathbf{x}}(k)\\) are standard normally distributed random variables, the estimation error \\( \tilde{\mathbf{x}} \\) is a standard normal random variable, too.
[Consult Wikipedia for more details.](https://en.wikipedia.org/wiki/Chi-squared_distribution)*

A very useful property of \\( \epsilon(k) \\) is that the expectation is

\\[E [ \epsilon(k) ] = n_x \. \\]

This can now be used to check whether a state estimator delivers consistent results, i.e. whether it correctly estimates the quality of its estimation.
Under the hypothesis \\( H_0 \\) that the filter is consistent, the hypothesis \\( H_0 \\) is accepted if

\\[ \epsilon (k) \in [r_1,r_2] \. \\]

The acceptance interval is chosen such that the probability that \\( H_0 \\) is accepted is \\( (1 - \alpha) \\) 

\\[ P \\{ \epsilon (k) \in [r_1,r_2]  \| H_0 \\} = 1 - \alpha \. \\]

If the NEES is greater than the upper bound \\( r_2 \\), then the filter estimates the error to be lower than it is in reality.
Provided that the measurement noise was selected properly, the process noise was chosen too low.
If the NEES is smaller than \\( r_1 \\), then the error is estimated larger than it really is. 
In this case the filter is called inefficient. 
The process noise was chosen too large.

Most often, one can test the hypothesis by plotting the \\( \text{NEES}( \epsilon(k) ) \\) and the confidence interval and check visualally if the NEES is inside of \\( [r_1,r_2] \\). 

<p align="center">
<img src="/assets/images/dc_motor/NEES.png" title="Normalized Estimation Error Squared (NEES)"/>
</p>

Quantitatively, one counts the number of NEES values that are outside the confidence interval and sets them in relation to the total number of \\(  \epsilon(k) \\). 
Formal speaking, 

\\[ \frac{1}{K} \sum_{ \forall k \in K : \epsilon(k) \in [r_1,r_2]} 1  < (1 - \alpha) \\]

where \\( K \in \mathbb{N}^+ \\) is the maximum number of time steps, e.g. \\( \{1,2,...,K \} \\).

To increase statistical significance, additional \\( N \\) independent samples of \\( \epsilon(k) \\) can be generated and an average NEES can be formed

\\[ \text{ANEES}( \epsilon(k) ) = \overline{\epsilon} (k)  = \frac{1}{N} \sum^N_{i=1} \epsilon_i(k) \. \\]

\\( \overline{\epsilon}\\) has the property that the expectation converges towards \\( N n_x \\) and that \\( \overline{\epsilon}\\) is chi-squared distributed with \\( N n_x \\) degrees of freedom.
The hypothesis test is repeated in the same way as described above.

Obviously, the NEES is statistically significant only when averaged over many runs.
This is easier to do in a simulation than in a real-world setting. 
In practice, it is therefore recommended to perform the [Normalized Innovation Squared (NIS)](/normalized-innovation-squared/) test exclusively or additionally to the NEES.


[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
