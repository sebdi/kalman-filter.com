---
layout: post
title:  "Normalized Estimation Error Squared (NEES)"
permalink: /normalized-estimation-error-squared/
date:   2022-09-05 00:00:00 +0000
categories: kalman-filter
---

A desired property of a state estimator is consistency. 
A state estimator is consistent if it is able to correctly indicate the quality of the estimate. 
It should be able to do this because an increase in sample size leads to a growth in information content and the state estimate \\( \hat{\mathbf{x}}(k) \\) is as close as possible to the true state \\( \mathbf{x}(k)  \\).
This results from the requirement that a state estimator shall be unbiased.
Mathematically speaking, this is expressed by the expected value of the estimation error \\( \tilde{\mathbf{x}}(k)  \\) being zero

\\[ E [ \mathbf{x}(k) - \hat{\mathbf{x}}(k)  ] = E [ \tilde{\mathbf{x}}(k)  ] = 0 \\]

and that the covariance matrix of the estimation error is equal to the covariance matrix of the state estimator

\\[ E [ [\mathbf{x}(k) - \hat{\mathbf{x}}(k)] [\mathbf{x}(k) - \hat{\mathbf{x}}(k)]^T ] = E [ \tilde{\mathbf{x}}(k\|k) \tilde{\mathbf{x}}(k\|k)^T  ] = \mathbf{P}(k\|k) \. \\]

Checking the consistency of the state estimator can be done with a hypothesis test using the Normalized Estimation Error Squared (NEES).
For this purpose, a normalization of the estimation error \\( \tilde{\mathbf{x}} \\) and the error covariance matrix \\( \mathbf{P} \\) is performed

\\[ \epsilon (k) = \tilde{\mathbf{x}}(k\|k)^T \mathbf{P}(k\|k)^{-1} \tilde{\mathbf{x}}(k\|k) \ .\\]

The quantity \\( \epsilon (k) \\)  is chi-squared distributed with \\( \text{dim}( \tilde{\mathbf{x}}(k) )) = n_x \\) degrees of freedom.

*Brief explanation why chi-squared distributed: The sum of squared random variables which are standard normally distributed are chi-squared distributed. 
Under the assumption that \\( \mathbf{x}(k) \\) and \\( \hat{\mathbf{x}}(k)\\) are standard normally distributed random variables, the estimation error \\( \tilde{\mathbf{x}} \\) is a standard normal random variable, too.
[Consult Wikipedia for more details.](https://en.wikipedia.org/wiki/Chi-squared_distribution)*

A very useful property of \\( \epsilon(k) \\) is that the expectation is

\\[E [ \epsilon(k) ] = n_x \. \\]

This can now be used to check whether a state estimator delivers consistent results, i.e. whether it correctly estimates the quality of its estimation.
Under the hypothesis \\( H_0 \\) that the filter is consistent, the hypothesis \\( H_0 \\) is accepted if

\\[ \epsilon (k) \in [r_1,r_2] \. \\]

The acceptance interval is chosen such that the probability that \\( H_0 \\) is accepted is \\( (1 - \alpha) \\) 

\\[ P \\{ \epsilon (k) \in [r_1,r_2]  \| H_0 \\} = 1 - \alpha \. \\]

If the NEES is greater than the upper bound \\( r_2 \\), then the filter estimates the error to be lower than it is in reality.
Provided that the measurement noise was selected properly, the process noise was chosen too low.
If the NEES is smaller than \\( r_1 \\), then the error is estimated larger than it really is. 
In this case the filter is called inefficient. 
The process noise was chosen too large.

Most often, one can test the hypothesis by plotting the \\( \text{NEES}( \epsilon(k) ) \\) and the confidence interval and check visualally if the NEES is inside of \\( [r_1,r_2] \\). 

<p align="center">
<img src="/assets/images/dc_motor/NEES.png" title="Normalized Estimation Error Squared (NEES)"/>
</p>

Quantitatively, one counts the number of NEES values that are outside the confidence interval and sets them in relation to the total number of \\(  \epsilon(k) \\). 
Formal speaking, 

\\[ \frac{1}{K} \sum_{ \forall k \in K : \epsilon(k) \in [r_1,r_2]} 1  < (1 - \alpha) \\]

where \\( K \in \mathbb{N}^+ \\) is the maximum number of time steps, e.g. \\( \{1,2,...,K \} \\).

To increase statistical significance, additional \\( N \\) independent samples of \\( \epsilon(k) \\) can be generated and an average NEES can be formed

\\[ \text{ANEES}( \epsilon(k) ) = \overline{\epsilon} (k)  = \frac{1}{N} \sum^N_{i=1} \epsilon_i(k) \. \\]

\\( \overline{\epsilon}\\) has the property that the expectation converges towards \\( N n_x \\) and that \\( \overline{\epsilon}\\) is chi-squared distributed with \\( N n_x \\) degrees of freedom.
The hypothesis test is repeated in the same way as described above.

Obviously, the NEES is only correct in a "truth model simulation", because the true state and the measurements are generated from the same system model with known statistical properties that the Kalman filter uses.
It is therefore likely that a Kalman filter will pass the NEES hypothesis test within the simulation, but not in the real application.
In practice, it is therefore recommended to perform the [Normalized Innovation Squared (NIS)](/normalized-innovation-squared/) test exclusively or additionally to the NEES, because it does not require access to the true state vector.
However, the NEES hypothesis test offers a deeper insight because the entire state vector is used and helps among other things in debugging within an application. 
This is based on the idea that if the implementation does not work correctly in a simulation with the "truth model", why should it work in reality.


[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/
